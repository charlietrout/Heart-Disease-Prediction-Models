[
  {
    "objectID": "Homework5.html",
    "href": "Homework5.html",
    "title": "Charlie Armentrout ST 558 Homework 5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model? &gt; The purpose of using cross-validation when fitting a random forest model is to assess its performance and generalizability by repeatedly training and evaluating the model on different subsets of the data. This helps in estimating how well the model will perform on new data that has not been seen before and aids in tuning its parameters to improve predictive accuracy without overfitting to the training data.\nDescribe the bagged tree algorithm. &gt; The bagged tree algorithm involves creating multiple bootstrap samples from the original dataset which are samples with replacement. The bootstrap samples must be the same size as the original dataset. With each bootstrap sample, you find a bootstrap statistic and then average this statistic out with all the other bootstrap statistics from the other bootstrap samples. This would be averaging for regression or what class has the most votes in classification as the final prediction.\nWhat is meant by a general linear model? &gt; A general linear model is a linear model that has a continuous response and allows for both continuous and categorical predictors.\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model? &gt; Adding an interaction term in a multiple linear regression model allows the model to account for how the relationship between one predictor and the dependent variable changes with different levels of another predictor. Without interaction terms, the model assumes that each predictor’s effect on the outcome is independent of the others.\nWhy do we split our data into a training and test set? &gt; We split our data into a training set and a test set to evaluate the performance of a machine learning model. The training set is used to train the model, adjusting its parameters to fit the data. The test set serves as an independent dataset to assess how well the trained model generalizes to new and not before seen data. This practice helps in detecting overfitting, where a model performs well on the training data but poorly on new data, and allows for tuning the model’s hyperparameters to ensure its reliability and effectiveness with new data."
  },
  {
    "objectID": "Homework5.html#quick-edadata-preparation",
    "href": "Homework5.html#quick-edadata-preparation",
    "title": "Charlie Armentrout ST 558 Homework 5",
    "section": "Quick EDA/Data Preparation",
    "text": "Quick EDA/Data Preparation\n\nSome basic EDA and data prep to delete bad observations before running models and checking performance\n\n\n# Load in required packages\nlibrary(caret)\nlibrary(dplyr)\nlibrary(readr)\nheart_data &lt;- read_csv(\"heart.csv\")\n# Quick data exploration and preparation\nsummary(heart_data)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   :  0.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.4  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   :  0.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:173.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :223.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :198.8   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:267.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:918         Min.   :-2.6000   Length:918         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.6000   Mode  :character   Median :1.0000  \n                    Mean   : 0.8874                      Mean   :0.5534  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\n# Check the structure of the dataset\nstr(heart_data)\n\nspc_tbl_ [918 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Age           : num [1:918] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex           : chr [1:918] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType : chr [1:918] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP     : num [1:918] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol   : num [1:918] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS     : num [1:918] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG    : chr [1:918] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR         : num [1:918] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina: chr [1:918] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak       : num [1:918] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ ST_Slope      : chr [1:918] \"Up\" \"Flat\" \"Up\" \"Flat\" ...\n $ HeartDisease  : num [1:918] 0 1 0 1 0 0 0 0 1 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Age = col_double(),\n  ..   Sex = col_character(),\n  ..   ChestPainType = col_character(),\n  ..   RestingBP = col_double(),\n  ..   Cholesterol = col_double(),\n  ..   FastingBS = col_double(),\n  ..   RestingECG = col_character(),\n  ..   MaxHR = col_double(),\n  ..   ExerciseAngina = col_character(),\n  ..   Oldpeak = col_double(),\n  ..   ST_Slope = col_character(),\n  ..   HeartDisease = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Check for missing values\nany(is.na(heart_data))  \n\n[1] FALSE\n\n# Correlation matrix for numeric variables\ncor_matrix &lt;- cor(select_if(heart_data, is.numeric))\nprint(cor_matrix)\n\n                     Age   RestingBP Cholesterol   FastingBS      MaxHR\nAge           1.00000000  0.25439936 -0.09528177  0.19803907 -0.3820447\nRestingBP     0.25439936  1.00000000  0.10089294  0.07019334 -0.1121350\nCholesterol  -0.09528177  0.10089294  1.00000000 -0.26097433  0.2357924\nFastingBS     0.19803907  0.07019334 -0.26097433  1.00000000 -0.1314385\nMaxHR        -0.38204468 -0.11213500  0.23579240 -0.13143849  1.0000000\nOldpeak       0.25861154  0.16480304  0.05014811  0.05269786 -0.1606906\nHeartDisease  0.28203851  0.10758898 -0.23274064  0.26729119 -0.4004208\n                 Oldpeak HeartDisease\nAge           0.25861154    0.2820385\nRestingBP     0.16480304    0.1075890\nCholesterol   0.05014811   -0.2327406\nFastingBS     0.05269786    0.2672912\nMaxHR        -0.16069055   -0.4004208\nOldpeak       1.00000000    0.4039507\nHeartDisease  0.40395072    1.0000000\n\n# Filter numeric variables\nnumeric_vars &lt;- select_if(heart_data, is.numeric)\n \n# Group by HeartDisease and compute mean and standard deviation\nsummary_stats &lt;- numeric_vars |&gt;\n     group_by(HeartDisease) |&gt;\n     summarise_all(list(mean = mean, sd = sd))\n# Print summary statistics\nprint(summary_stats)\n\n# A tibble: 2 × 13\n  HeartDisease Age_mean RestingBP_mean Cholesterol_mean FastingBS_mean\n         &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1            0     50.6           130.             227.          0.107\n2            1     55.9           134.             176.          0.335\n# ℹ 8 more variables: MaxHR_mean &lt;dbl&gt;, Oldpeak_mean &lt;dbl&gt;, Age_sd &lt;dbl&gt;,\n#   RestingBP_sd &lt;dbl&gt;, Cholesterol_sd &lt;dbl&gt;, FastingBS_sd &lt;dbl&gt;,\n#   MaxHR_sd &lt;dbl&gt;, Oldpeak_sd &lt;dbl&gt;\n\n# Drops all obs where Cholesterol = 0 or RestingBP = 0 as this can affect models later on if these predictors are used\nheart_data &lt;- heart_data[heart_data$Cholesterol != 0 & heart_data$RestingBP != 0, ]\n\n\nTurn HeartDisease Variable into factor so tree and logistic regression models can run properly and drop ST_Slope variable as it is irrelevant\n\n\n# Create a factor version of HeartDisease variable\nheart_data$HeartDisease &lt;- as.factor(heart_data$HeartDisease)\n# Remove ST_Slope variable\nheart_data &lt;- select(heart_data, -ST_Slope)\n\n\nCreation of dummy variables here is so kNN model can run properly as kNN needs all numeric predictors generally\n\n\n# Select only the categorical predictors\ncategorical_vars &lt;- select(heart_data, Sex, ExerciseAngina, ChestPainType, RestingECG)\n\n# Create dummy variables using dummyVars from caret\ndummy_formula &lt;- \"~ .\"\n\n# ~ . specifies that all columns in the data here should be transformed into dummy variables\ndummy_data &lt;- dummyVars(dummy_formula, data = categorical_vars)\n\n# Apply the transformation to create dummy variables\ndummy_transformed &lt;- predict(dummy_data, newdata = categorical_vars)\n\n# Add the dummy variables to the original dataset\nheart_data &lt;- cbind(heart_data, dummy_transformed)\n\n# Remove the original categorical variables from the dataset\nheart_data &lt;- select(heart_data, -Sex, -ExerciseAngina, -ChestPainType, -RestingECG)\n\n# Verifying the changes\nstr(heart_data)\n\n'data.frame':   746 obs. of  18 variables:\n $ Age             : num  40 49 37 48 54 39 45 54 37 48 ...\n $ RestingBP       : num  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol     : num  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MaxHR           : num  172 156 98 108 122 170 170 142 130 120 ...\n $ Oldpeak         : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDisease    : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n $ SexF            : num  0 1 0 1 0 0 1 0 0 1 ...\n $ SexM            : num  1 0 1 0 1 1 0 1 1 0 ...\n $ ExerciseAnginaN : num  1 1 1 0 1 1 1 1 0 1 ...\n $ ExerciseAnginaY : num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeASY: num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeATA: num  1 0 1 0 0 0 1 1 0 1 ...\n $ ChestPainTypeNAP: num  0 1 0 0 1 1 0 0 0 0 ...\n $ ChestPainTypeTA : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGLVH   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGNormal: num  1 1 0 1 1 1 1 1 1 1 ...\n $ RestingECGST    : num  0 0 1 0 0 0 0 0 0 0 ..."
  },
  {
    "objectID": "Homework5.html#split-your-data",
    "href": "Homework5.html#split-your-data",
    "title": "Charlie Armentrout ST 558 Homework 5",
    "section": "Split your Data",
    "text": "Split your Data\nBasic training set and test set split to prep for model testing\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Split data into training (70%) and test (30%) sets\ntrain_indices &lt;- createDataPartition(heart_data$HeartDisease, p = 0.7, list = FALSE)\ntrain_data &lt;- heart_data[train_indices, ]\ntest_data &lt;- heart_data[-train_indices, ]\n\n# Check the dimensions of the training and test sets making sure rows add up to 918\ndim(train_data)\n\n[1] 523  18\n\ndim(test_data)\n\n[1] 223  18"
  },
  {
    "objectID": "Homework5.html#knn",
    "href": "Homework5.html#knn",
    "title": "Charlie Armentrout ST 558 Homework 5",
    "section": "kNN",
    "text": "kNN\nRunning and eval of kNN model\n\n# Fit kNN model using only numeric predictors\nknn_fit &lt;- train(\n  HeartDisease ~ .,\n  data = train_data,\n  method = \"knn\",\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  tuneGrid = expand.grid(k = 1:40)\n)\n\n# Print best model parameters\nprint(knn_fit)\n\nk-Nearest Neighbors \n\n523 samples\n 17 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 471, 470, 471, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7737663  0.5458911\n   2  0.7629294  0.5239218\n   3  0.7972545  0.5931839\n   4  0.8087808  0.6160541\n   5  0.8214683  0.6419881\n   6  0.8113087  0.6218601\n   7  0.8190131  0.6373686\n   8  0.8183720  0.6358337\n   9  0.8164731  0.6318413\n  10  0.8145259  0.6279234\n  11  0.8196299  0.6382460\n  12  0.8196299  0.6382611\n  13  0.8189889  0.6370035\n  14  0.8164127  0.6320449\n  15  0.8209845  0.6412053\n  16  0.8158563  0.6309206\n  17  0.8114538  0.6220946\n  18  0.8095428  0.6184299\n  19  0.8069908  0.6134571\n  20  0.8075955  0.6147490\n  21  0.8076318  0.6147879\n  22  0.8057088  0.6107711\n  23  0.8095670  0.6189114\n  24  0.8108249  0.6215736\n  25  0.8095791  0.6190132\n  26  0.8115143  0.6228415\n  27  0.8089260  0.6178791\n  28  0.8012579  0.6023106\n  29  0.8038220  0.6075732\n  30  0.8006289  0.6013015\n  31  0.8019110  0.6038643\n  32  0.8012700  0.6024966\n  33  0.8044630  0.6090059\n  34  0.8063861  0.6128212\n  35  0.8044630  0.6090589\n  36  0.8063740  0.6128376\n  37  0.8038341  0.6078178\n  38  0.8051040  0.6103826\n  39  0.8051161  0.6104165\n  40  0.8051282  0.6105247\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 5.\n\n# Evaluate on test set using only numeric predictors\nknn_pred &lt;- predict(knn_fit, newdata = test_data)\nconfusionMatrix(knn_pred, test_data$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 100  25\n         1  17  81\n                                          \n               Accuracy : 0.8117          \n                 95% CI : (0.7541, 0.8608)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6211          \n                                          \n Mcnemar's Test P-Value : 0.2801          \n                                          \n            Sensitivity : 0.8547          \n            Specificity : 0.7642          \n         Pos Pred Value : 0.8000          \n         Neg Pred Value : 0.8265          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4484          \n   Detection Prevalence : 0.5605          \n      Balanced Accuracy : 0.8094          \n                                          \n       'Positive' Class : 0               \n                                          \n\ncm_knn &lt;- confusionMatrix(knn_pred, test_data$HeartDisease)\nprint(cm_knn)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 100  25\n         1  17  81\n                                          \n               Accuracy : 0.8117          \n                 95% CI : (0.7541, 0.8608)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6211          \n                                          \n Mcnemar's Test P-Value : 0.2801          \n                                          \n            Sensitivity : 0.8547          \n            Specificity : 0.7642          \n         Pos Pred Value : 0.8000          \n         Neg Pred Value : 0.8265          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4484          \n   Detection Prevalence : 0.5605          \n      Balanced Accuracy : 0.8094          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#logistic-regression",
    "href": "Homework5.html#logistic-regression",
    "title": "Charlie Armentrout ST 558 Homework 5",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nRunning and eval of various logistic regression model\n\n#Resetting the heart_data dataset back to its original state with no dummy variables first\nheart_data &lt;- read_csv(\"heart.csv\")\nheart_data &lt;- heart_data[heart_data$Cholesterol != 0 & heart_data$RestingBP != 0, ]\n# Create a factor version of HeartDisease variable\nheart_data$HeartDisease &lt;- as.factor(heart_data$HeartDisease)\n# Remove ST_Slope variable\nheart_data &lt;- select(heart_data, -ST_Slope)\n# Set seed for reproducibility\nset.seed(123)\n\n# Split data into training (70%) and test (30%) sets\ntrain_indices &lt;- createDataPartition(heart_data$HeartDisease, p = 0.7, list = FALSE)\ntrain_data &lt;- heart_data[train_indices, ]\ntest_data &lt;- heart_data[-train_indices, ]\n# Fit logistic regression models\nmodel1 &lt;- train(\n  HeartDisease ~ .,\n  data = train_data,\n  method = \"glm\",\n  family = binomial,\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n)\n\nmodel2 &lt;- train(\n  HeartDisease ~ Age + Sex + RestingBP + MaxHR,\n  data = train_data,\n  method = \"glm\",\n  family = binomial,\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n)\n\nmodel3 &lt;- train(\n  HeartDisease ~ Age * Sex * RestingBP * MaxHR,\n  data = train_data,\n  method = \"glm\",\n  family = binomial,\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n)\n\n# Print model summaries\nprint(model1)\n\nGeneralized Linear Model \n\n523 samples\n 10 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 471, 470, 471, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7960571  0.5910411\n\nprint(model2)\n\nGeneralized Linear Model \n\n523 samples\n  4 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 471, 471, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7298984  0.4587894\n\nprint(model3)\n\nGeneralized Linear Model \n\n523 samples\n  4 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 470, 471, 471, 471, 471, 471, ... \nResampling results:\n\n  Accuracy  Kappa    \n  0.719642  0.4396758\n\n# Evaluate model 1 on test set\npred_model1 &lt;- predict(model1, newdata = test_data)\n\n\n# Confusion matrix for model 1, the best one\ncm_model1 &lt;- confusionMatrix(pred_model1, test_data$HeartDisease)\n\n# Print confusion matrix of model 1\nprint(cm_model1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 106  28\n         1  11  78\n                                          \n               Accuracy : 0.8251          \n                 95% CI : (0.7688, 0.8726)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.6467          \n                                          \n Mcnemar's Test P-Value : 0.01041         \n                                          \n            Sensitivity : 0.9060          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7910          \n         Neg Pred Value : 0.8764          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4753          \n   Detection Prevalence : 0.6009          \n      Balanced Accuracy : 0.8209          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#tree-models",
    "href": "Homework5.html#tree-models",
    "title": "Charlie Armentrout ST 558 Homework 5",
    "section": "Tree Models",
    "text": "Tree Models\nRunning and eval of various tree models\n\n# Fit classification tree model\ntree_model &lt;- train(\n  HeartDisease ~ .,\n  data = train_data,\n  method = \"rpart\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001))\n)\n\n# Print best model parameters\nprint(tree_model)\n\nCART \n\n523 samples\n 10 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 470, 470, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7633769  0.5264310\n  0.001  0.7633769  0.5264310\n  0.002  0.7646468  0.5288708\n  0.003  0.7684446  0.5363913\n  0.004  0.7729318  0.5455812\n  0.005  0.7811805  0.5621059\n  0.006  0.7811684  0.5616625\n  0.007  0.7817852  0.5625867\n  0.008  0.7753749  0.5490853\n  0.009  0.7753991  0.5488233\n  0.010  0.7753991  0.5488233\n  0.011  0.7773222  0.5525378\n  0.012  0.7748186  0.5477656\n  0.013  0.7748186  0.5477656\n  0.014  0.7722424  0.5427739\n  0.015  0.7722424  0.5427739\n  0.016  0.7728713  0.5440449\n  0.017  0.7735123  0.5453616\n  0.018  0.7767296  0.5518002\n  0.019  0.7767296  0.5518002\n  0.020  0.7805394  0.5596143\n  0.021  0.7805394  0.5596143\n  0.022  0.7805394  0.5596143\n  0.023  0.7767296  0.5520092\n  0.024  0.7767296  0.5520092\n  0.025  0.7767296  0.5520092\n  0.026  0.7767296  0.5520092\n  0.027  0.7728834  0.5445601\n  0.028  0.7728834  0.5445601\n  0.029  0.7716255  0.5419035\n  0.030  0.7716255  0.5419035\n  0.031  0.7716255  0.5419035\n  0.032  0.7665215  0.5316725\n  0.033  0.7665215  0.5316725\n  0.034  0.7658805  0.5301683\n  0.035  0.7658805  0.5301683\n  0.036  0.7608128  0.5190243\n  0.037  0.7608128  0.5190243\n  0.038  0.7608128  0.5190243\n  0.039  0.7608128  0.5190243\n  0.040  0.7652879  0.5273186\n  0.041  0.7652879  0.5273186\n  0.042  0.7652879  0.5273186\n  0.043  0.7652879  0.5272804\n  0.044  0.7652879  0.5272804\n  0.045  0.7703919  0.5370420\n  0.046  0.7703919  0.5370420\n  0.047  0.7703919  0.5370420\n  0.048  0.7703919  0.5370420\n  0.049  0.7703919  0.5370420\n  0.050  0.7703919  0.5370420\n  0.051  0.7703919  0.5370420\n  0.052  0.7703919  0.5370420\n  0.053  0.7703919  0.5370420\n  0.054  0.7703919  0.5370420\n  0.055  0.7703919  0.5370420\n  0.056  0.7703919  0.5370420\n  0.057  0.7703919  0.5370420\n  0.058  0.7703919  0.5370420\n  0.059  0.7703919  0.5370420\n  0.060  0.7703919  0.5370420\n  0.061  0.7703919  0.5370420\n  0.062  0.7703919  0.5370420\n  0.063  0.7703919  0.5370420\n  0.064  0.7703919  0.5370420\n  0.065  0.7703919  0.5370420\n  0.066  0.7703919  0.5370420\n  0.067  0.7703919  0.5370420\n  0.068  0.7703919  0.5370420\n  0.069  0.7703919  0.5370420\n  0.070  0.7703919  0.5370420\n  0.071  0.7703919  0.5370420\n  0.072  0.7703919  0.5370420\n  0.073  0.7703919  0.5370420\n  0.074  0.7703919  0.5370420\n  0.075  0.7703919  0.5370420\n  0.076  0.7703919  0.5370420\n  0.077  0.7703919  0.5370420\n  0.078  0.7703919  0.5370420\n  0.079  0.7703919  0.5370420\n  0.080  0.7703919  0.5370420\n  0.081  0.7703919  0.5370420\n  0.082  0.7703919  0.5370420\n  0.083  0.7703919  0.5370420\n  0.084  0.7703919  0.5370420\n  0.085  0.7703919  0.5370420\n  0.086  0.7703919  0.5370420\n  0.087  0.7703919  0.5370420\n  0.088  0.7703919  0.5370420\n  0.089  0.7703919  0.5370420\n  0.090  0.7703919  0.5370420\n  0.091  0.7703919  0.5370420\n  0.092  0.7703919  0.5370420\n  0.093  0.7703919  0.5370420\n  0.094  0.7703919  0.5370420\n  0.095  0.7703919  0.5370420\n  0.096  0.7703919  0.5370420\n  0.097  0.7703919  0.5370420\n  0.098  0.7703919  0.5370420\n  0.099  0.7703919  0.5370420\n  0.100  0.7703919  0.5370420\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.007.\n\n# Evaluate on test set\ntree_pred &lt;- predict(tree_model, newdata = test_data)\ncm_tree &lt;- confusionMatrix(tree_pred, test_data$HeartDisease)\nprint(cm_tree)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 95 25\n         1 22 81\n                                          \n               Accuracy : 0.7892          \n                 95% CI : (0.7298, 0.8408)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 2.267e-16       \n                                          \n                  Kappa : 0.5769          \n                                          \n Mcnemar's Test P-Value : 0.7705          \n                                          \n            Sensitivity : 0.8120          \n            Specificity : 0.7642          \n         Pos Pred Value : 0.7917          \n         Neg Pred Value : 0.7864          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4260          \n   Detection Prevalence : 0.5381          \n      Balanced Accuracy : 0.7881          \n                                          \n       'Positive' Class : 0               \n                                          \n\n# Fit random forest model\nrf_model &lt;- train(\n  HeartDisease ~ .,\n  data = train_data,\n  method = \"rf\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  tuneGrid = expand.grid(mtry = 1:ncol(train_data) - 1)  # Try values from 1 to number of predictors\n)\n\n# Print best model parameters\nprint(rf_model)\n\nRandom Forest \n\n523 samples\n 10 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 470, 471, 471, 471, 471, 470, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   0    0.8088413  0.6152296\n   1    0.8056483  0.6091227\n   2    0.7986454  0.5952366\n   3    0.7999516  0.5985339\n   4    0.7935897  0.5858555\n   5    0.7859700  0.5705252\n   6    0.7897678  0.5783705\n   7    0.7859579  0.5706471\n   8    0.7776609  0.5542546\n   9    0.7802129  0.5594306\n  10    0.7763546  0.5517058\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 0.\n\n# Evaluate on test set\nrf_pred &lt;- predict(rf_model, newdata = test_data)\ncm_rf &lt;- confusionMatrix(rf_pred, test_data$HeartDisease)\nprint(cm_rf)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 111  36\n         1   6  70\n                                          \n               Accuracy : 0.8117          \n                 95% CI : (0.7541, 0.8608)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6173          \n                                          \n Mcnemar's Test P-Value : 7.648e-06       \n                                          \n            Sensitivity : 0.9487          \n            Specificity : 0.6604          \n         Pos Pred Value : 0.7551          \n         Neg Pred Value : 0.9211          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4978          \n   Detection Prevalence : 0.6592          \n      Balanced Accuracy : 0.8045          \n                                          \n       'Positive' Class : 0               \n                                          \n\n# Fit boosted tree model\ngbm_grid &lt;- expand.grid(\n  n.trees = c(25, 50, 100, 200),\n  interaction.depth = c(1, 2, 3),\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\ngbm_model &lt;- train(\n  HeartDisease ~ .,\n  data = train_data,\n  method = \"gbm\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  tuneGrid = gbm_grid,\n  verbose = FALSE\n)\n\n# Print best model parameters\nprint(gbm_model)\n\nStochastic Gradient Boosting \n\n523 samples\n 10 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 471, 471, 471, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.8035438  0.6052770\n  1                   50      0.8137518  0.6258356\n  1                  100      0.8182390  0.6352710\n  1                  200      0.8099419  0.6189912\n  2                   25      0.8137760  0.6258536\n  2                   50      0.8131592  0.6249724\n  2                  100      0.8169569  0.6326725\n  2                  200      0.8176584  0.6344151\n  3                   25      0.8106193  0.6195435\n  3                   50      0.8112361  0.6212316\n  3                  100      0.8164006  0.6319210\n  3                  200      0.8157596  0.6303459\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 100, interaction.depth =\n 1, shrinkage = 0.1 and n.minobsinnode = 10.\n\n# Evaluate on test set\ngbm_pred &lt;- predict(gbm_model, newdata = test_data)\ncm_gbm &lt;- confusionMatrix(gbm_pred, test_data$HeartDisease)\nprint(cm_gbm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 112  30\n         1   5  76\n                                          \n               Accuracy : 0.843           \n                 95% CI : (0.7885, 0.8882)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6818          \n                                          \n Mcnemar's Test P-Value : 4.976e-05       \n                                          \n            Sensitivity : 0.9573          \n            Specificity : 0.7170          \n         Pos Pred Value : 0.7887          \n         Neg Pred Value : 0.9383          \n             Prevalence : 0.5247          \n         Detection Rate : 0.5022          \n   Detection Prevalence : 0.6368          \n      Balanced Accuracy : 0.8371          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#wrap-up",
    "href": "Homework5.html#wrap-up",
    "title": "Charlie Armentrout ST 558 Homework 5",
    "section": "Wrap Up",
    "text": "Wrap Up\nComparison of overall accuracies from confusion matrices of kNN model, top logistic regression model and the 3 tree models\n\n# Print overall model accuracies\nprint(\"Model Accuracies:\")\n\n[1] \"Model Accuracies:\"\n\nprint(paste(\"kNN:\", round(cm_knn$overall['Accuracy'], 4),\n            \"Logistic Regression:\", round(cm_model1$overall['Accuracy'], 4),\n            \"Classification Tree:\", round(cm_tree$overall['Accuracy'], 4),\n            \"Random Forest:\", round(cm_rf$overall['Accuracy'], 4),\n            \"Boosted Tree:\", round(cm_gbm$overall['Accuracy'], 4)))\n\n[1] \"kNN: 0.8117 Logistic Regression: 0.8251 Classification Tree: 0.7892 Random Forest: 0.8117 Boosted Tree: 0.843\"\n\n\nThe model that did the best job in terms of accuracy on the test set was the boosted tree model"
  }
]